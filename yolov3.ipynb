{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d21d476-e48b-406f-933e-341150f78e53",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d389fb1-c8c0-4bf3-bca7-65207ca2f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "from flame.core.data.pascal_dataset import PascalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b39881-dd52-4bc8-85bc-cdf05f34b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "imsize = 416\n",
    "VOC2012 = {\n",
    "    'image_dir': '../efficient_det_pytorch/dataset/PASCALVOC2012/JPEGImages/',\n",
    "    'label_dir': '../efficient_det_pytorch/dataset/PASCALVOC2012/Annotations/',\n",
    "    'txt_path': '../efficient_det_pytorch/dataset/PASCALVOC2012/ImageSets/Segmentation/train.txt',\n",
    "}\n",
    "\n",
    "VOC2007 = {\n",
    "    'image_dir': '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/',\n",
    "    'label_dir': '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/',\n",
    "}\n",
    "image_extent = '.jpg'\n",
    "label_extent = '.xml'\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "classes = {\n",
    "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n",
    "    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n",
    "    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19,\n",
    "}\n",
    "S = [13, 26, 52]  # image_size // 32, image_size // 16, image_size // 8\n",
    "C = 20\n",
    "anchors = [[[0.28, 0.22], [0.38, 0.48], [0.9, 0.78]],  # S = 13\n",
    "           [[0.07, 0.15], [0.15, 0.11], [0.14, 0.29]],  # S = 26\n",
    "           [[0.02, 0.03], [0.04, 0.07], [0.08, 0.06]]]  # S = 52\n",
    "transforms = [\n",
    "    iaa.Fliplr(p=0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd17eee6-85d8-46bd-a7c9-f68945b30eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train:\n",
      "\t VOC2007: 5011\n",
      "\t VOC2012: 1464\n",
      "\t Total: 6475\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PascalDataset(\n",
    "    VOC2007=VOC2007, VOC2012=VOC2012,\n",
    "    image_extent=image_extent, label_extent=label_extent,\n",
    "    classes=classes, mean=mean, std=std,\n",
    "    imsize=imsize, S=S, anchors=anchors, transforms=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034be504-52a6-4e7b-af2e-aa9816539dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43235723-ef86-43f2-8843-4a79d3782793",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db151696-ddd0-4e8f-9f0a-b0d4c9c23873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from flame.core.data.visualize import to_image, draw_box\n",
    "\n",
    "idx2class = {idx: class_name for class_name, idx in classes.items()}\n",
    "\n",
    "iter_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08f7cb6c-e1ad-4687-a525-3fc9b01a8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, targets, infos = iter_loader.next()\n",
    "\n",
    "s1, s2, s3 = targets  # s1, s2, s3: N x 3 x S x S x 6\n",
    "\n",
    "for sample, target in zip(samples, s3):  # choose s1 or s2 or s3\n",
    "    grid_size = sample.shape[2] / target.shape[2]\n",
    "\n",
    "    image = to_image(sample)\n",
    "\n",
    "    indices = target[:, :, :, 0] == 1  # 3 x 13 x 13 x 6\n",
    "    boxes = target[indices]  # n_boxes x 6\n",
    "\n",
    "    x = (target[..., 1:2] == boxes[:, 1]).nonzero(as_tuple=True)[2]  # columns\n",
    "    y = (target[..., 2:3] == boxes[:, 2]).nonzero(as_tuple=True)[1]  # rows\n",
    "    \n",
    "    boxes[:, 1] += x\n",
    "    boxes[:, 2] += y\n",
    "    boxes[:, 1:5] *= grid_size\n",
    "    boxes[:, [1, 2]] -= boxes[:, [3, 4]] / 2  # x1, y1 = x - w / 2, y - h / 2\n",
    "    boxes[:, [3, 4]] += boxes[:, [1, 2]]  # x2 = x1 + w, y2 = y1 + h\n",
    "\n",
    "    boxes = boxes.to(torch.int32).cpu().numpy().tolist()\n",
    "\n",
    "    for box in boxes:\n",
    "        draw_box(\n",
    "            image, box=box[1:5], name=idx2class[box[5]],\n",
    "            box_color=(0, 0, 255), text_color=(255, 0, 0)\n",
    "        )\n",
    "        cv2.imshow(f'visual S={target.shape[2]}', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36e3a7-01c9-463a-9c78-84261de2adec",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247ff34-d382-4f61-95fc-f35a53765c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb6b5fe-560f-441e-9cc1-c00bcb26c94c",
   "metadata": {},
   "source": [
    "## Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3c4aa7-9411-4359-a6ca-d1792e96be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for anchor generation\n",
    "image_size = (416 * 3, 416 * 3)  # w, h\n",
    "\n",
    "# all predicting scales\n",
    "scales = [13, 26, 52]\n",
    "\n",
    "# width and height of each anchor boxes at each scales\n",
    "anchors = {\n",
    "    13: [[116, 90], [156, 198], [373, 326]],\n",
    "    26: [[30, 61], [62, 45], [59, 119]],\n",
    "    52: [[10, 13], [16, 30], [33, 23]]\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8420d126-bafd-4155-9f52-6fcae108cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 13, 13, 4])\n"
     ]
    }
   ],
   "source": [
    "S = scales[0]\n",
    "anchor = anchors[S]\n",
    "\n",
    "import torch\n",
    "\n",
    "anchor_boxes = {}\n",
    "\n",
    "for scale in scales:\n",
    "    grid_size_x, grid_size_y = (image_size[0] / S, image_size[1] / S)\n",
    "\n",
    "    anchor_sizes = torch.tensor(anchors[scale], dtype=torch.float, device=device)  # 3 x 2\n",
    "\n",
    "    w = anchor_sizes[:, 0].view(3, 1, 1)  # 3 x 1 x 1\n",
    "    h = anchor_sizes[:, 1].view(3, 1, 1)  # 3 x 1 x 1\n",
    "\n",
    "    cx = torch.arange(start=grid_size_x / 2, end=image_size[0], step=grid_size_x)  # scale\n",
    "    cy = torch.arange(start=grid_size_y / 2, end=image_size[1], step=grid_size_y)  # scale\n",
    "\n",
    "    cx, cy = torch.meshgrid(cx, cy)  # cx: scale x scale, cy: scale x scale  (coordinates)\n",
    "    cx, cy = cx.unsqueeze(dim=0), cy.unsqueeze(dim=0)  # 1 x scale x scale\n",
    "\n",
    "    x1, y1 = cx - w / 2, cy - h / 2  # 3 x scale x scale\n",
    "    x2, y2 = cx + w / 2, cy + h / 2  # 3 x scale x scale\n",
    "\n",
    "    boxes = torch.stack([x1, y1, x2, y2], dim=3)  # 3 x scale x scale x 4\n",
    "    \n",
    "    anchor_boxes[scale] = boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13edc2c9-68ce-42da-98b4-b1a015496314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = np.zeros(shape=(image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "all_boxes = anchor_boxes.numpy().reshape(3, -1, 4)\n",
    "\n",
    "for boxes in all_boxes:\n",
    "    for box in boxes:\n",
    "        box = np.int32(box)\n",
    "        cv2.rectangle(\n",
    "            img=image,\n",
    "            pt1=tuple(box[:2]),\n",
    "            pt2=tuple(box[2:]),\n",
    "            color=(0, 0, 255),\n",
    "            thickness=1\n",
    "        )\n",
    "        cv2.circle(\n",
    "            img=image,\n",
    "            center=(int((box[0] + box[2]) / 2),\n",
    "                    int((box[1] + box[3]) / 2)),\n",
    "            radius=1,\n",
    "            color=(0, 255, 0),\n",
    "            thickness=-1\n",
    "        )\n",
    "    cv2.imshow('a', image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43678ff2-6e88-4a61-9098-eff79b916e44",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa369ca7-10f0-4187-8d5b-b1682036fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride) \n",
    "Every conv is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],  # To this point is Darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            bias=not bn_act,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.leaky = nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8504b415-c58b-44c7-ad0f-ab3350060462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "model = YOLOv3(num_classes=20)\n",
    "\n",
    "dummy_tensor = torch.randn((2, 3, 416, 416))\n",
    "\n",
    "outputs = model(dummy_tensor)\n",
    "\n",
    "assert outputs[0].shape == (2, 3, 416 // 32, 416 // 32, 20 + 5)\n",
    "assert outputs[1].shape == (2, 3, 416 // 16, 416 // 16, 20 + 5)\n",
    "assert outputs[2].shape == (2, 3, 416 // 8, 416 // 8, 20 + 5)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac7ab6-0ed2-4ace-ab94-39568ccb0c08",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640ee5b-65d2-42fc-a49f-2347324c5804",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c71fda6-1fd6-40fc-90e6-da0a4679c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def _resize(image: np.ndarray, imsize=416) -> np.ndarray:\n",
    "    ratio = imsize / max(image.shape)\n",
    "    image = cv2.resize(image, (0, 0), fx=ratio, fy=ratio)\n",
    "    return image\n",
    "\n",
    "def _pad_to_square(image: np.ndarray) -> np.ndarray:\n",
    "    height, width = image.shape[:2]\n",
    "    max_size = max(height, width)\n",
    "    image = np.pad(image, ((0, max_size - height), (0, max_size - width), (0, 0)))\n",
    "    return image\n",
    "\n",
    "def preprocess(images, imsize=416, mean=[0, 0, 0], std=[1, 1, 1], device='cpu'):\n",
    "    mean = torch.tensor(mean, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(std, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    samples = [_resize(image, imsize=imsize) for image in images]  # resize\n",
    "    samples = [_pad_to_square(sample) for sample in samples]  # pad to square\n",
    "    samples = [cv2.cvtColor(sample, cv2.COLOR_BGR2RGB) for sample in samples]  # BGR -> RGB\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]  # array -> torch\n",
    "    samples = torch.stack(samples, dim=0).to(device)  # stack\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "\n",
    "    scales = [max(image.shape[:2]) / imsize for image in images]\n",
    "\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b4592-0645-4dfc-93bd-e0febcad2968",
   "metadata": {},
   "source": [
    "## postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e819d4ff-17c1-43f1-bbe1-ab9ef0b1994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import ops\n",
    "from typing import Tuple, List\n",
    "\n",
    "# postprocessing\n",
    "anchors = [[(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],    # S = 13\n",
    "           [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],   # S = 26\n",
    "           [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],]  # S = 52\n",
    "\n",
    "def inference(\n",
    "        predictions: Tuple[torch.Tensor],\n",
    "        anchors: List[List[Tuple[float, float]]],\n",
    "        image_size: int = 416,\n",
    "        iou_threshold: float = 0.5,\n",
    "        score_threshold: float = 0.5\n",
    "    ):\n",
    "    '''get all boxes at gird S x S (grid_size = image_size / S)\n",
    "    Args:\n",
    "        preds: Tuple[[N x 3 x S x S x (tp, tx, ty, tw, th, n_classes)]] with S = [13, 26, 52]\n",
    "        anchors: [3 x 3 x 2] (pw, ph with size in [0, 1])  (relative to image_size)\n",
    "    Outputs:\n",
    "        scores: [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "        labels: [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "        bboxes: [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3)) x 4], with [x1 y1 x2 y2].\n",
    "    '''\n",
    "    device = predictions[0].device\n",
    "    batch_size = predictions[0].shape[0]\n",
    "\n",
    "    batch_boxes, batch_labels, batch_scores = [], [], []\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        S = pred.shape[2]  # 13, 26, 52\n",
    "\n",
    "        # anchor: 1 x 3 x 1 x 1 x 2\n",
    "        anchor = torch.tensor(anchors[i], device=device, dtype=torch.float)  # anchor: 3 x 2\n",
    "        anchor = anchor.reshape(1, 3, 1, 1, 2)\n",
    "\n",
    "        # cx, cy: N x 3 x S x S\n",
    "        cx = torch.arange(S).repeat(batch_size, 3, S, 1).to(device)\n",
    "        cy = cx.permute(0, 1, 3, 2)\n",
    "\n",
    "        # N x 3 x S x S -> reshape: N x (3 * S * S)\n",
    "        # score = sigmoid(tp)\n",
    "        scores = torch.sigmoid(pred[..., 0]).reshape(batch_size, -1)\n",
    "\n",
    "        # N x 3 x S x S -> reshape: N x (3 * S * S)\n",
    "        labels = torch.argmax(pred[..., 5:], dim=-1).reshape(batch_size, -1)\n",
    "\n",
    "        # xy: N x 3 x S x S x 2 (center of bboxes)\n",
    "        # bx = sigmoid(tx) + cx, by = sigmoid(ty) + cy\n",
    "        bx = (torch.sigmoid(pred[..., 1]) + cx) * (image_size / S)\n",
    "        by = (torch.sigmoid(pred[..., 2]) + cy) * (image_size / S)\n",
    "        bxy = torch.stack([bx, by], dim=-1)\n",
    "\n",
    "        # wh: N x 3 x S x S x 2 (width, height of bboxes)\n",
    "        # bw = pw * e ^ tw, bh = ph * e ^ th\n",
    "        bwh = (image_size * anchor) * torch.exp(pred[..., 3:5])\n",
    "\n",
    "        # boxes (x1 y1 x2 y2 type): N x (3 * S * S) x 4\n",
    "        boxes = torch.cat([bxy - bwh / 2, bxy + bwh / 2], dim=-1).reshape(batch_size, -1, 4)\n",
    "        boxes = torch.clamp(boxes, min=0, max=image_size)\n",
    "\n",
    "        batch_boxes.append(boxes)\n",
    "        batch_labels.append(labels)\n",
    "        batch_scores.append(scores)\n",
    "\n",
    "    batch_labels = torch.cat(batch_labels, dim=1)  # [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "    batch_scores = torch.cat(batch_scores, dim=1)  # [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "    batch_boxes = torch.cat(batch_boxes, dim=1)  # [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3)) x 4]\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        score_indices = batch_scores[batch_id, :] > score_threshold\n",
    "\n",
    "        if score_indices.sum() == 0:\n",
    "            predictions.append(\n",
    "                {\n",
    "                    'boxes': torch.tensor([[0, 0, 1, 1]], dtype=torch.float, device=device),\n",
    "                    'labels': torch.tensor([-1], dtype=torch.int64, device=device),\n",
    "                    'scores': torch.tensor([0], dtype=torch.float, device=device)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            continue\n",
    "\n",
    "        boxes = batch_boxes[batch_id, score_indices, :]  # n_boxes x 4\n",
    "        labels = batch_labels[batch_id, score_indices]  # n_labels\n",
    "        scores = batch_scores[batch_id, score_indices]  # n_scores\n",
    "\n",
    "        nms_indices = ops.boxes.batched_nms(\n",
    "            boxes=boxes, scores=scores, idxs=labels,\n",
    "            iou_threshold=iou_threshold\n",
    "        )\n",
    "\n",
    "        if nms_indices.shape[0] != 0:\n",
    "            predictions.append(\n",
    "                {\n",
    "                    'boxes': boxes[nms_indices, :],\n",
    "                    'labels': labels[nms_indices],\n",
    "                    'scores': scores[nms_indices]\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            predictions.append(\n",
    "                {\n",
    "                    'boxes': torch.tensor([[0, 0, 1, 1]], dtype=torch.float, device=device),\n",
    "                    'labels': torch.tensor([-1], dtype=torch.int64, device=device),\n",
    "                    'scores': torch.tensor([0], dtype=torch.float, device=device)\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdd8388-946f-433e-809f-6140d64bbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained weight\n",
    "model = YOLOv3(num_classes=20)\n",
    "state_dict = torch.load(f='checkpoint/pretrained_weight/78.1map_0.2threshold_PASCAL.tar', map_location='cpu')\n",
    "model.load_state_dict(state_dict=state_dict['state_dict'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd8ec13-9522-43ee-b564-43db239762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import cv2\n",
    "image_paths = [\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000012.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000016.jpg',\n",
    "]\n",
    "\n",
    "images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "\n",
    "images, scales, samples = preprocess(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a191a66-9ec9-461a-9aa7-f3f8963e6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([3, 3, 416, 416])\n",
      "Output Shape at S=13: torch.Size([3, 3, 13, 13, 25])\n",
      "Output Shape at S=26: torch.Size([3, 3, 26, 26, 25])\n",
      "Output Shape at S=52: torch.Size([3, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "with torch.no_grad():\n",
    "    preds = model(samples)\n",
    "\n",
    "print(f'Input Shape: {samples.shape}')    \n",
    "print(f'Output Shape at S={preds[0].shape[2]}: {preds[0].shape}')\n",
    "print(f'Output Shape at S={preds[1].shape[2]}: {preds[1].shape}')\n",
    "print(f'Output Shape at S={preds[2].shape[2]}: {preds[2].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b05a451-896a-42a7-9349-842c6d2fc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post processing\n",
    "\n",
    "predictions = inference(\n",
    "    predictions=preds,\n",
    "    anchors=anchors,\n",
    "    image_size=416,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b52bfbe-22dc-4cbe-be81-7ea9289c2280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[  9.4115,  11.4099, 237.5261, 397.4678],\n",
       "          [178.3522, 178.9196, 293.7810, 403.9183],\n",
       "          [  3.0243, 170.0002, 227.6837, 416.0000],\n",
       "          [177.9559,  20.7553, 292.5116, 388.3447],\n",
       "          [  0.0000,   6.5021, 101.8935, 352.0404],\n",
       "          [  0.0000, 190.1166,  91.4532, 402.8295],\n",
       "          [194.3734,  22.5865, 275.5323, 272.7602]]),\n",
       "  'labels': tensor([14,  1,  1, 14, 14,  1, 14]),\n",
       "  'scores': tensor([0.8944, 0.8688, 0.8530, 0.8383, 0.8261, 0.8107, 0.5676])},\n",
       " {'boxes': tensor([[126.4862,  70.2691, 286.6494, 227.7021]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.8998])},\n",
       " {'boxes': tensor([[ 72.8772,  36.9785, 282.7292, 398.4085]]),\n",
       "  'labels': tensor([1]),\n",
       "  'scores': tensor([0.8798])}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df470f41-bb44-4306-b568-67ab2b7371d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "classes = list(classes2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25cbf58d-770c-4b96-9714-b28e5aa2c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    thickness = max(image.shape) // 500\n",
    "    fontscale = max(image.shape) / 500\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes = (boxes * scale).astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "#         if label != -1:\n",
    "        cv2.rectangle(\n",
    "            img=image,\n",
    "            pt1=tuple(box[:2]),\n",
    "            pt2=tuple(box[2:]),    \n",
    "            color=color,\n",
    "            thickness=thickness\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            img=image,\n",
    "            text=f'{class_name}: {score: .4f}',\n",
    "            org=tuple(box[:2]),\n",
    "            fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "            fontScale=fontscale,\n",
    "            color=color,\n",
    "            thickness=thickness,\n",
    "            lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(class_name, image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d53420-64dc-4679-b9aa-272f2f6cbdef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
