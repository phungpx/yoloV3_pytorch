{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d21d476-e48b-406f-933e-341150f78e53",
   "metadata": {},
   "source": [
    "# 1. DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d46212-4da3-4795-b24c-98d2fa7a49df",
   "metadata": {},
   "source": [
    "## 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d389fb1-c8c0-4bf3-bca7-65207ca2f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "from flame.core.data.pascal_dataset import PascalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b39881-dd52-4bc8-85bc-cdf05f34b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "VOC2012 = {\n",
    "    'image_dir': '../efficient_det_pytorch/dataset/PASCALVOC2012/JPEGImages/',\n",
    "    'label_dir': '../efficient_det_pytorch/dataset/PASCALVOC2012/Annotations/',\n",
    "    'txt_path': '../efficient_det_pytorch/dataset/PASCALVOC2012/ImageSets/Segmentation/train.txt',\n",
    "}\n",
    "\n",
    "VOC2007 = {\n",
    "    'image_dir': '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/',\n",
    "    'label_dir': '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/',\n",
    "}\n",
    "image_size = 416\n",
    "image_extent = '.jpg'\n",
    "label_extent = '.xml'\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "classes = {\n",
    "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n",
    "    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n",
    "    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19,\n",
    "}\n",
    "transforms = [\n",
    "    iaa.Fliplr(p=0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd17eee6-85d8-46bd-a7c9-f68945b30eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train:\n",
      "\t VOC2007: 5011\n",
      "\t VOC2012: 1464\n",
      "\t Total: 6475\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PascalDataset(\n",
    "    VOC2007=VOC2007, VOC2012=VOC2012,\n",
    "    image_extent=image_extent, label_extent=label_extent,\n",
    "    classes=classes, mean=mean, std=std,\n",
    "    image_size=image_size, transforms=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac91cb-054d-49c0-b242-1d0bb3e3168b",
   "metadata": {},
   "source": [
    "## 1.2 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034be504-52a6-4e7b-af2e-aa9816539dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch:tuple(zip(*batch)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43235723-ef86-43f2-8843-4a79d3782793",
   "metadata": {},
   "source": [
    "## 1.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db151696-ddd0-4e8f-9f0a-b0d4c9c23873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from flame.core.data.visualize import to_image, draw_box\n",
    "\n",
    "idx2class = {idx: class_name for class_name, idx in classes.items()}\n",
    "iter_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838d6a95-90bc-4229-b71e-c223a5ccfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, targets, image_infos = iter_loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2649e38c-4f7d-4da7-8d75-ee5acc7fcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "for sample, target, image_info in zip(samples, targets, image_infos):\n",
    "    image = to_image(sample)\n",
    "    boxes = target['boxes'].to(torch.int32).data.cpu().numpy().tolist()\n",
    "    labels = target['labels'].data.cpu().numpy().tolist()\n",
    "    for label, box in zip(labels, boxes):\n",
    "        draw_box(\n",
    "            image=image, box=box, name=idx2class[label]\n",
    "        )\n",
    "    cv2.imshow(Path(image_info[0]).name, image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f7cb6c-e1ad-4687-a525-3fc9b01a8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 shape: torch.Size([2, 3, 13, 13, 6])\n",
      "s2 shape: torch.Size([2, 3, 26, 26, 6])\n",
      "s3 shape: torch.Size([2, 3, 52, 52, 6])\n"
     ]
    }
   ],
   "source": [
    "s1, s2, s3 = targets  # s1, s2, s3: N x 3 x S x S x 6\n",
    "print(f's1 shape: {s1.shape}')\n",
    "print(f's2 shape: {s2.shape}')\n",
    "print(f's3 shape: {s3.shape}')\n",
    "\n",
    "for sample, target in zip(samples, s3):  # choose s1 or s2 or s3\n",
    "    grid_size = sample.shape[2] / target.shape[2]\n",
    "\n",
    "    image = to_image(sample)\n",
    "\n",
    "    indices = target[:, :, :, 0] == 1  # 3 x 13 x 13 x 6\n",
    "    boxes = target[indices]  # n_boxes x 6\n",
    "\n",
    "    x = (target[..., 1:2] == boxes[:, 1]).nonzero(as_tuple=True)[2]  # columns\n",
    "    y = (target[..., 2:3] == boxes[:, 2]).nonzero(as_tuple=True)[1]  # rows\n",
    "    \n",
    "    boxes[:, 1] += x\n",
    "    boxes[:, 2] += y\n",
    "    boxes[:, 1:5] *= grid_size\n",
    "    boxes[:, [1, 2]] -= boxes[:, [3, 4]] / 2  # x1, y1 = x - w / 2, y - h / 2\n",
    "    boxes[:, [3, 4]] += boxes[:, [1, 2]]  # x2 = x1 + w, y2 = y1 + h\n",
    "\n",
    "    boxes = boxes.to(torch.int32).cpu().numpy().tolist()\n",
    "\n",
    "    for box in boxes:\n",
    "        draw_box(\n",
    "            image, box=box[1:5], name=idx2class[box[5]],\n",
    "            box_color=(0, 0, 255), text_color=(255, 0, 0)\n",
    "        )\n",
    "        cv2.imshow(f'visual S={target.shape[2]}', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36e3a7-01c9-463a-9c78-84261de2adec",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6247ff34-d382-4f61-95fc-f35a53765c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 13, 13, 25])\n",
      "torch.Size([2, 3, 26, 26, 25])\n",
      "torch.Size([2, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flame.core.model.model import Model\n",
    "\n",
    "model = Model(\n",
    "    in_channels=3,\n",
    "    num_classes=20,\n",
    "    weight_path='checkpoint/PASCAL/2110011416/best_model_30_loss=-1.4638.pt',\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.5,\n",
    "    anchors = [\n",
    "      [[116, 90], [156, 198], [373, 326]],  # S = 13\n",
    "      [[30, 61], [62, 45], [59, 119]],      # S = 26\n",
    "      [[10, 13], [16, 30], [33, 23]],       # S = 52\n",
    "    ]\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.train().to(device)\n",
    "\n",
    "dummy_tensor = torch.FloatTensor(2, 3, 416, 416, device=device)\n",
    "\n",
    "preds = model(dummy_tensor)\n",
    "\n",
    "print(preds[0].shape)  # N x 3 x S x S x (5 + C)\n",
    "print(preds[1].shape)\n",
    "print(preds[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6b5fe-560f-441e-9cc1-c00bcb26c94c",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c96a95e9-ecbb-476e-97a6-7866553ad16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.stack(samples, dim=0)\n",
    "preds = model(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8bef04f-c135-45a5-82c2-b360422e3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.loss.yolov3_loss import YOLOv3Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ffb05a9-d529-4672-9d16-4fd5cd969c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = YOLOv3Loss(\n",
    "    lambda_obj=1,\n",
    "    lambda_noobj=10,\n",
    "    lambda_bbox=1,\n",
    "    lambda_class=1,\n",
    "    image_size=416,\n",
    "    scales=[13, 26, 52],\n",
    "    anchor_sizes=[\n",
    "      [[116, 90], [156, 198], [373, 326]],  # S = 13\n",
    "      [[30, 61], [62, 45], [59, 119]],      # S = 26\n",
    "      [[10, 13], [16, 30], [33, 23]],       # S = 52,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "928d4b67-bbb9-4f5f-b77a-60044e6d4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phungpx/anaconda3/envs/phungpx/lib/python3.7/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([12, 12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{13: tensor(12.7649, grad_fn=<AddBackward0>),\n",
       " 26: tensor(13.2871, grad_fn=<AddBackward0>),\n",
       " 52: tensor(16.5258, grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac7ab6-0ed2-4ace-ab94-39568ccb0c08",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640ee5b-65d2-42fc-a49f-2347324c5804",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c71fda6-1fd6-40fc-90e6-da0a4679c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def _resize(image: np.ndarray, imsize=416) -> np.ndarray:\n",
    "    ratio = imsize / max(image.shape)\n",
    "    image = cv2.resize(image, (0, 0), fx=ratio, fy=ratio)\n",
    "    return image\n",
    "\n",
    "def _pad_to_square(image: np.ndarray) -> np.ndarray:\n",
    "    height, width = image.shape[:2]\n",
    "    max_size = max(height, width)\n",
    "    image = np.pad(image, ((0, max_size - height), (0, max_size - width), (0, 0)))\n",
    "    return image\n",
    "\n",
    "def preprocess(images, imsize=416, mean=[0, 0, 0], std=[1, 1, 1], device='cpu'):\n",
    "    mean = torch.tensor(mean, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(std, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    samples = [_resize(image, imsize=imsize) for image in images]  # resize\n",
    "    samples = [_pad_to_square(sample) for sample in samples]  # pad to square\n",
    "    samples = [cv2.cvtColor(sample, cv2.COLOR_BGR2RGB) for sample in samples]  # BGR -> RGB\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]  # array -> torch\n",
    "    samples = torch.stack(samples, dim=0).to(device)  # stack\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "\n",
    "    scales = [max(image.shape[:2]) / imsize for image in images]\n",
    "\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b1f87-8fa8-4040-b7bf-c43f7b61b2e7",
   "metadata": {},
   "source": [
    "## 4.2 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd8388-946f-433e-809f-6140d64bbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flame.core.model.model import Model\n",
    "\n",
    "model = Model(\n",
    "    in_channels=3,\n",
    "    num_classes=20,\n",
    "    weight_path=None,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.5,\n",
    "    anchors = [\n",
    "      [[116, 90], [156, 198], [373, 326]],  # S = 13\n",
    "      [[30, 61], [62, 45], [59, 119]],      # S = 26\n",
    "      [[10, 13], [16, 30], [33, 23]],       # S = 52\n",
    "    ]\n",
    ")\n",
    "\n",
    "weight_path = 'checkpoint/pretrained_weight/78.1map_0.2threshold_PASCAL.tar'\n",
    "state_dict = torch.load(f=weight_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict=state_dict['state_dict'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd8ec13-9522-43ee-b564-43db239762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import cv2\n",
    "image_paths = [\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000005.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000012.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000016.jpg',\n",
    "]\n",
    "\n",
    "images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "\n",
    "images, scales, samples = preprocess(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a191a66-9ec9-461a-9aa7-f3f8963e6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "with torch.no_grad():\n",
    "    predictions = model.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b52bfbe-22dc-4cbe-be81-7ea9289c2280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[135.9501, 214.1852, 221.9094, 306.2251],\n",
       "          [204.0586, 162.0790, 262.1912, 271.0089],\n",
       "          [  3.7298, 196.1347,  64.0971, 306.6113],\n",
       "          [229.0565, 154.5116, 262.0498, 182.1058],\n",
       "          [261.2530, 150.9323, 289.4495, 178.0880],\n",
       "          [194.4179, 162.5784, 245.5823, 246.1445],\n",
       "          [179.7555, 159.3967, 226.5517, 216.8225],\n",
       "          [216.3913, 162.4353, 269.0368, 230.1576],\n",
       "          [231.6277, 153.7339, 253.8889, 174.0897],\n",
       "          [279.6370, 153.8638, 305.1120, 176.5713],\n",
       "          [377.8582, 157.4993, 413.6432, 313.5740],\n",
       "          [253.8978, 156.4704, 282.8614, 184.9576],\n",
       "          [378.5215, 148.7126, 413.3721, 307.0582],\n",
       "          [266.7535, 156.9698, 286.7319, 182.9836],\n",
       "          [220.2417, 156.5490, 265.9907, 201.1744]]),\n",
       "  'labels': tensor([ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 14,  8,  8]),\n",
       "  'scores': tensor([0.8458, 0.8184, 0.8114, 0.7521, 0.7256, 0.6656, 0.6551, 0.6264, 0.5843,\n",
       "          0.5616, 0.5572, 0.5501, 0.5443, 0.5423, 0.5120])},\n",
       " {'boxes': tensor([[127.5399,  70.9313, 285.5956, 227.0399]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.8998])},\n",
       " {'boxes': tensor([[ 73.2695,  36.1319, 282.3368, 399.2551]]),\n",
       "  'labels': tensor([1]),\n",
       "  'scores': tensor([0.8798])}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df470f41-bb44-4306-b568-67ab2b7371d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "classes = list(classes2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25cbf58d-770c-4b96-9714-b28e5aa2c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    thickness = max(image.shape) // 500\n",
    "    fontscale = max(image.shape) / 500\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes = (boxes * scale).astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "#         if label != -1:\n",
    "        cv2.rectangle(\n",
    "            img=image,\n",
    "            pt1=tuple(box[:2]),\n",
    "            pt2=tuple(box[2:]),    \n",
    "            color=color,\n",
    "            thickness=thickness\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            img=image,\n",
    "            text=f'{class_name}: {score: .4f}',\n",
    "            org=tuple(box[:2]),\n",
    "            fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "            fontScale=fontscale,\n",
    "            color=color,\n",
    "            thickness=thickness,\n",
    "            lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(class_name, image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13714f3b-947c-43db-a28a-dab641685649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
