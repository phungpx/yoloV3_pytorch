{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d21d476-e48b-406f-933e-341150f78e53",
   "metadata": {},
   "source": [
    "# 1. DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d46212-4da3-4795-b24c-98d2fa7a49df",
   "metadata": {},
   "source": [
    "## 1.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d389fb1-c8c0-4bf3-bca7-65207ca2f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "from torch.utils.data import DataLoader\n",
    "from flame.core.data.pascal_dataset import PascalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b39881-dd52-4bc8-85bc-cdf05f34b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "imsize = 416\n",
    "VOC2012 = {\n",
    "    'image_dir': '../efficient_det_pytorch/dataset/PASCALVOC2012/JPEGImages/',\n",
    "    'label_dir': '../efficient_det_pytorch/dataset/PASCALVOC2012/Annotations/',\n",
    "    'txt_path': '../efficient_det_pytorch/dataset/PASCALVOC2012/ImageSets/Segmentation/train.txt',\n",
    "}\n",
    "\n",
    "VOC2007 = {\n",
    "    'image_dir': '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/',\n",
    "    'label_dir': '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/',\n",
    "}\n",
    "image_extent = '.jpg'\n",
    "label_extent = '.xml'\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "classes = {\n",
    "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n",
    "    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n",
    "    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19,\n",
    "}\n",
    "S = [13, 26, 52]  # image_size // 32, image_size // 16, image_size // 8\n",
    "anchors = [\n",
    "    [[116, 90], [156, 198], [373, 326]],  # S = 13\n",
    "    [[30, 61], [62, 45], [59, 119]],      # S = 26\n",
    "    [[10, 13], [16, 30], [33, 23]],       # S = 52\n",
    "]\n",
    "transforms = [\n",
    "    iaa.Fliplr(p=0.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd17eee6-85d8-46bd-a7c9-f68945b30eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train:\n",
      "\t VOC2007: 5011\n",
      "\t VOC2012: 1464\n",
      "\t Total: 6475\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PascalDataset(\n",
    "    VOC2007=VOC2007, VOC2012=VOC2012,\n",
    "    image_extent=image_extent, label_extent=label_extent,\n",
    "    classes=classes, mean=mean, std=std,\n",
    "    imsize=imsize, S=S, anchors=anchors, transforms=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac91cb-054d-49c0-b242-1d0bb3e3168b",
   "metadata": {},
   "source": [
    "## 1.2 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034be504-52a6-4e7b-af2e-aa9816539dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch:tuple(zip(*batch)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43235723-ef86-43f2-8843-4a79d3782793",
   "metadata": {},
   "source": [
    "## 1.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db151696-ddd0-4e8f-9f0a-b0d4c9c23873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from flame.core.data.visualize import to_image, draw_box\n",
    "\n",
    "idx2class = {idx: class_name for class_name, idx in classes.items()}\n",
    "\n",
    "iter_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838d6a95-90bc-4229-b71e-c223a5ccfb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, targets, image_infos, boxes_infos = iter_loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fca0b214-0458-4489-8d11-67d838afe39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 shape: torch.Size([2, 3, 13, 13, 6])\n",
      "s2 shape: torch.Size([2, 3, 26, 26, 6])\n",
      "s3 shape: torch.Size([2, 3, 52, 52, 6])\n"
     ]
    }
   ],
   "source": [
    "targets = [torch.stack(target, dim=0).to('cpu') for target in zip(*targets)]\n",
    "print(f's1 shape: {targets[0].shape}')\n",
    "print(f's2 shape: {targets[1].shape}')\n",
    "print(f's3 shape: {targets[2].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b8f7ef9-782d-4969-ac13-5df909917d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.stack(samples, dim=0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5445e9e6-3a36-4105-8f28-f4f03e51b669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../efficient_det_pytorch/dataset/PASCALVOC2012/JPEGImages/2011_002300.jpg',\n",
       "  (500, 375)],\n",
       " ['../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/009762.jpg',\n",
       "  (500, 375)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f7cb6c-e1ad-4687-a525-3fc9b01a8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 shape: torch.Size([2, 3, 13, 13, 6])\n",
      "s2 shape: torch.Size([2, 3, 26, 26, 6])\n",
      "s3 shape: torch.Size([2, 3, 52, 52, 6])\n"
     ]
    }
   ],
   "source": [
    "s1, s2, s3 = targets  # s1, s2, s3: N x 3 x S x S x 6\n",
    "print(f's1 shape: {s1.shape}')\n",
    "print(f's2 shape: {s2.shape}')\n",
    "print(f's3 shape: {s3.shape}')\n",
    "\n",
    "for sample, target in zip(samples, s3):  # choose s1 or s2 or s3\n",
    "    grid_size = sample.shape[2] / target.shape[2]\n",
    "\n",
    "    image = to_image(sample)\n",
    "\n",
    "    indices = target[:, :, :, 0] == 1  # 3 x 13 x 13 x 6\n",
    "    boxes = target[indices]  # n_boxes x 6\n",
    "\n",
    "    x = (target[..., 1:2] == boxes[:, 1]).nonzero(as_tuple=True)[2]  # columns\n",
    "    y = (target[..., 2:3] == boxes[:, 2]).nonzero(as_tuple=True)[1]  # rows\n",
    "    \n",
    "    boxes[:, 1] += x\n",
    "    boxes[:, 2] += y\n",
    "    boxes[:, 1:5] *= grid_size\n",
    "    boxes[:, [1, 2]] -= boxes[:, [3, 4]] / 2  # x1, y1 = x - w / 2, y - h / 2\n",
    "    boxes[:, [3, 4]] += boxes[:, [1, 2]]  # x2 = x1 + w, y2 = y1 + h\n",
    "\n",
    "    boxes = boxes.to(torch.int32).cpu().numpy().tolist()\n",
    "\n",
    "    for box in boxes:\n",
    "        draw_box(\n",
    "            image, box=box[1:5], name=idx2class[box[5]],\n",
    "            box_color=(0, 0, 255), text_color=(255, 0, 0)\n",
    "        )\n",
    "        cv2.imshow(f'visual S={target.shape[2]}', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18bb2f20-131b-4e13-b5c9-855df9cc8cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['../efficient_det_pytorch/dataset/PASCALVOC2012/JPEGImages/2011_002300.jpg',\n",
       "  (500, 375)],\n",
       " ['../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/009762.jpg',\n",
       "  (500, 375)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "381a2a32-3671-4c55-96cd-3fc9c3662b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ANCHORS = np.array([\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "])  # Note these have been rescaled to be between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64fd1de8-d7d5-476d-a75f-45dcf39a7b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[116.48,  91.52],\n",
       "        [158.08, 199.68],\n",
       "        [374.4 , 324.48]],\n",
       "\n",
       "       [[ 29.12,  62.4 ],\n",
       "        [ 62.4 ,  45.76],\n",
       "        [ 58.24, 120.64]],\n",
       "\n",
       "       [[  8.32,  12.48],\n",
       "        [ 16.64,  29.12],\n",
       "        [ 33.28,  24.96]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANCHORS * 416"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36e3a7-01c9-463a-9c78-84261de2adec",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6247ff34-d382-4f61-95fc-f35a53765c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 13, 13, 25])\n",
      "torch.Size([2, 3, 26, 26, 25])\n",
      "torch.Size([2, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flame.core.model.model import Model\n",
    "\n",
    "model = Model(\n",
    "    in_channels=3,\n",
    "    num_classes=20,\n",
    "    weight_path='checkpoint/pretrained_weight/78.1map_0.2threshold_PASCAL.tar',\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.5,\n",
    "    anchors = [\n",
    "      [[116, 90], [156, 198], [373, 326]],  # S = 13\n",
    "      [[30, 61], [62, 45], [59, 119]],      # S = 26\n",
    "      [[10, 13], [16, 30], [33, 23]],       # S = 52\n",
    "    ]\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.train().to(device)\n",
    "\n",
    "dummy_tensor = torch.FloatTensor(2, 3, 416, 416, device=device)\n",
    "\n",
    "preds = model(dummy_tensor)\n",
    "\n",
    "print(preds[0].shape)  # N x 3 x S x S x (5 + C)\n",
    "print(preds[1].shape)\n",
    "print(preds[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6b5fe-560f-441e-9cc1-c00bcb26c94c",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b872b5-c702-45a9-9c4e-d5865cfabaaa",
   "metadata": {},
   "source": [
    "## 3.1 Anchor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb3c4aa7-9411-4359-a6ca-d1792e96be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for anchor generation\n",
    "image_size = (416, 416)  # w, h\n",
    "\n",
    "# all predicting scales\n",
    "scales = [13, 26, 52]\n",
    "\n",
    "# width and height of each anchor boxes at each scales\n",
    "anchors = {\n",
    "    13: [[116, 90], [156, 198], [373, 326]],\n",
    "    26: [[30, 61], [62, 45], [59, 119]],\n",
    "    52: [[10, 13], [16, 30], [33, 23]]\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8420d126-bafd-4155-9f52-6fcae108cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "anchor_boxes = {}\n",
    "\n",
    "for scale in scales:\n",
    "    grid_size_x, grid_size_y = (image_size[0] / scale, image_size[1] / scale)\n",
    "\n",
    "    anchor_sizes = torch.tensor(anchors[scale], dtype=torch.float, device=device)  # 3 x 2\n",
    "\n",
    "    w = anchor_sizes[:, 0].view(3, 1, 1)  # 3 x 1 x 1\n",
    "    h = anchor_sizes[:, 1].view(3, 1, 1)  # 3 x 1 x 1\n",
    "\n",
    "    cx = torch.arange(start=grid_size_x / 2, end=image_size[0], step=grid_size_x)  # scale\n",
    "    cy = torch.arange(start=grid_size_y / 2, end=image_size[1], step=grid_size_y)  # scale\n",
    "\n",
    "    cx, cy = torch.meshgrid(cx, cy)  # cx: scale x scale, cy: scale x scale  (coordinates)\n",
    "    cx, cy = cx.unsqueeze(dim=0), cy.unsqueeze(dim=0)  # 1 x scale x scale\n",
    "\n",
    "    x1, y1 = cx - w / 2, cy - h / 2  # 3 x scale x scale\n",
    "    x2, y2 = cx + w / 2, cy + h / 2  # 3 x scale x scale\n",
    "\n",
    "    boxes = torch.stack([x1, y1, x2, y2], dim=3)  # 3 x scale x scale x 4\n",
    "    \n",
    "    anchor_boxes[scale] = boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c8b6b-62a8-4875-b16e-3c23785d4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test anchor generation for each scales\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = np.zeros(shape=(image_size[0], image_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "for scale in scales:\n",
    "    all_boxes = anchor_boxes[scale].numpy().reshape(3, -1, 4)\n",
    "    for boxes in all_boxes:\n",
    "        for box in boxes:\n",
    "            box = np.int32(box)\n",
    "            cv2.rectangle(\n",
    "                img=image,\n",
    "                pt1=tuple(box[:2]),\n",
    "                pt2=tuple(box[2:]),\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1\n",
    "            )\n",
    "            cv2.circle(\n",
    "                img=image,\n",
    "                center=(int((box[0] + box[2]) / 2),\n",
    "                        int((box[1] + box[3]) / 2)),\n",
    "                radius=1,\n",
    "                color=(0, 255, 0),\n",
    "                thickness=-1\n",
    "            )\n",
    "        cv2.imshow('a', image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "050841bb-4793-41e6-af99-9406e1f2f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors at Scale Shape: torch.Size([3, 13, 13, 4])\n",
      "Groundtruth Boxes: torch.Size([2, 4])\n",
      "IoUs: torch.Size([3, 13, 13, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_iou(anchors: torch.Tensor, boxes: torch.Tensor) -> torch.Tensor:\n",
    "    '''compute IoU between each anchor boxes and groundtruth boxes\n",
    "    Args:\n",
    "        anchors: [num_anchors_per_scale, S, S, 4], S = 13 or 26 or 52\n",
    "        box_type: [y1, x1, y2, x2]\n",
    "        boxes: [num_boxes, 4]\n",
    "        box_type: [x1, y1, x2, y2]\n",
    "    Output:\n",
    "        ious: [num_anchors_per_scale, S, S, num_boxes]\n",
    "    references: https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics\n",
    "    '''\n",
    "    # get params and reshape anchors form [3 x S x S x 4] to [(3 * S * S), 4]\n",
    "    num_boxes = boxes.shape[0]\n",
    "    num_anchors_per_scale, S = anchors.shape[0], anchors.shape[1]\n",
    "    anchors = anchors.reshape(num_anchors_per_scale * S * S, 4)  # num_anchors_per_scale * S * S, 4\n",
    "\n",
    "    # calculate intersection areas of anchors and target boxes\n",
    "    # num_anchors = num_anchors_per_scale * S * S\n",
    "    inter_width = torch.min(anchors[:, 3].unsqueeze(dim=1), boxes[:, 2]) - torch.max(anchors[:, 1].unsqueeze(dim=1), boxes[:, 0])\n",
    "    inter_height = torch.min(anchors[:, 2].unsqueeze(dim=1), boxes[:, 3]) - torch.max(anchors[:, 0].unsqueeze(dim=1), boxes[:, 1])\n",
    "    inter_width = torch.clamp(inter_width, min=0.)  # num_anchors x num_boxes\n",
    "    inter_height = torch.clamp(inter_height, min=0.)  # num_anchors x num_boxes\n",
    "    inter_areas = inter_width * inter_height  # num_anchors x num_boxes\n",
    "\n",
    "    # calculate union areas of anchors and target boxes\n",
    "    # num_anchors = num_anchors_per_scale * S * S\n",
    "    area_anchors = (anchors[:, 2] - anchors[:, 0]) * (anchors[:, 3] - anchors[:, 1])  # num_anchors\n",
    "    area_boxes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])  # num_boxes\n",
    "    union_areas = area_anchors.unsqueeze(dim=1) + area_boxes - inter_width * inter_height  # num_anchors x num_boxes\n",
    "    union_areas = torch.clamp(union_areas, min=1e-8)\n",
    "\n",
    "    # calculate ious of anchors and target boxes\n",
    "    # shape of ious is [(num_anchors_per_scale * S * S) x num_boxes]\n",
    "    ious = inter_areas / union_areas\n",
    "\n",
    "    # reshape ious from [(num_anchors_per_scale * S * S) x num_boxes] to [num_anchors_per_scale x S x S x num_boxes]\n",
    "    ious = ious.reshape(num_anchors_per_scale, S, S, num_boxes)\n",
    "\n",
    "    return ious\n",
    "\n",
    "# test compute iou\n",
    "anchors = torch.FloatTensor(3, 13, 13, 4)\n",
    "boxes = torch.FloatTensor(2, 4)\n",
    "ious = compute_iou(anchors, boxes)\n",
    "\n",
    "print(f'Anchors at Scale Shape: {anchors.shape}')\n",
    "print(f'Groundtruth Boxes: {boxes.shape}')\n",
    "print(f'IoUs: {ious.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac7ab6-0ed2-4ace-ab94-39568ccb0c08",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640ee5b-65d2-42fc-a49f-2347324c5804",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c71fda6-1fd6-40fc-90e6-da0a4679c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def _resize(image: np.ndarray, imsize=416) -> np.ndarray:\n",
    "    ratio = imsize / max(image.shape)\n",
    "    image = cv2.resize(image, (0, 0), fx=ratio, fy=ratio)\n",
    "    return image\n",
    "\n",
    "def _pad_to_square(image: np.ndarray) -> np.ndarray:\n",
    "    height, width = image.shape[:2]\n",
    "    max_size = max(height, width)\n",
    "    image = np.pad(image, ((0, max_size - height), (0, max_size - width), (0, 0)))\n",
    "    return image\n",
    "\n",
    "def preprocess(images, imsize=416, mean=[0, 0, 0], std=[1, 1, 1], device='cpu'):\n",
    "    mean = torch.tensor(mean, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(std, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    samples = [_resize(image, imsize=imsize) for image in images]  # resize\n",
    "    samples = [_pad_to_square(sample) for sample in samples]  # pad to square\n",
    "    samples = [cv2.cvtColor(sample, cv2.COLOR_BGR2RGB) for sample in samples]  # BGR -> RGB\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]  # array -> torch\n",
    "    samples = torch.stack(samples, dim=0).to(device)  # stack\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "\n",
    "    scales = [max(image.shape[:2]) / imsize for image in images]\n",
    "\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b1f87-8fa8-4040-b7bf-c43f7b61b2e7",
   "metadata": {},
   "source": [
    "## 4.2 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdd8388-946f-433e-809f-6140d64bbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flame.core.model.model import Model\n",
    "\n",
    "model = Model(\n",
    "    in_channels=3,\n",
    "    num_classes=20,\n",
    "    weight_path=None,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.5,\n",
    "    anchors = [\n",
    "      [[116, 90], [156, 198], [373, 326]],  # S = 13\n",
    "      [[30, 61], [62, 45], [59, 119]],      # S = 26\n",
    "      [[10, 13], [16, 30], [33, 23]],       # S = 52\n",
    "    ]\n",
    ")\n",
    "\n",
    "weight_path = 'checkpoint/pretrained_weight/78.1map_0.2threshold_PASCAL.tar'\n",
    "state_dict = torch.load(f=weight_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict=state_dict['state_dict'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd8ec13-9522-43ee-b564-43db239762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import cv2\n",
    "image_paths = [\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000005.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000012.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000016.jpg',\n",
    "]\n",
    "\n",
    "images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "\n",
    "images, scales, samples = preprocess(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a191a66-9ec9-461a-9aa7-f3f8963e6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "with torch.no_grad():\n",
    "    predictions = model.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b52bfbe-22dc-4cbe-be81-7ea9289c2280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[135.9501, 214.1852, 221.9094, 306.2251],\n",
       "          [204.0586, 162.0790, 262.1912, 271.0089],\n",
       "          [  3.7298, 196.1347,  64.0971, 306.6113],\n",
       "          [229.0565, 154.5116, 262.0498, 182.1058],\n",
       "          [261.2530, 150.9323, 289.4495, 178.0880],\n",
       "          [194.4179, 162.5784, 245.5823, 246.1445],\n",
       "          [179.7555, 159.3967, 226.5517, 216.8225],\n",
       "          [216.3913, 162.4353, 269.0368, 230.1576],\n",
       "          [231.6277, 153.7339, 253.8889, 174.0897],\n",
       "          [279.6370, 153.8638, 305.1120, 176.5713],\n",
       "          [377.8582, 157.4993, 413.6432, 313.5740],\n",
       "          [253.8978, 156.4704, 282.8614, 184.9576],\n",
       "          [378.5215, 148.7126, 413.3721, 307.0582],\n",
       "          [266.7535, 156.9698, 286.7319, 182.9836],\n",
       "          [220.2417, 156.5490, 265.9907, 201.1744]]),\n",
       "  'labels': tensor([ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 14,  8,  8]),\n",
       "  'scores': tensor([0.8458, 0.8184, 0.8114, 0.7521, 0.7256, 0.6656, 0.6551, 0.6264, 0.5843,\n",
       "          0.5616, 0.5572, 0.5501, 0.5443, 0.5423, 0.5120])},\n",
       " {'boxes': tensor([[127.5399,  70.9313, 285.5956, 227.0399]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.8998])},\n",
       " {'boxes': tensor([[ 73.2695,  36.1319, 282.3368, 399.2551]]),\n",
       "  'labels': tensor([1]),\n",
       "  'scores': tensor([0.8798])}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df470f41-bb44-4306-b568-67ab2b7371d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "classes = list(classes2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25cbf58d-770c-4b96-9714-b28e5aa2c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    thickness = max(image.shape) // 500\n",
    "    fontscale = max(image.shape) / 500\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes = (boxes * scale).astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "#         if label != -1:\n",
    "        cv2.rectangle(\n",
    "            img=image,\n",
    "            pt1=tuple(box[:2]),\n",
    "            pt2=tuple(box[2:]),    \n",
    "            color=color,\n",
    "            thickness=thickness\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            img=image,\n",
    "            text=f'{class_name}: {score: .4f}',\n",
    "            org=tuple(box[:2]),\n",
    "            fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "            fontScale=fontscale,\n",
    "            color=color,\n",
    "            thickness=thickness,\n",
    "            lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(class_name, image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13714f3b-947c-43db-a28a-dab641685649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
