{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e1310c-b6a6-4ab5-b284-9f66ab54221f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa369ca7-10f0-4187-8d5b-b1682036fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride) \n",
    "Every conv is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],  # To this point is Darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8504b415-c58b-44c7-ad0f-ab3350060462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "model = YOLOv3(num_classes=20)\n",
    "\n",
    "dummy_tensor = torch.randn((2, 3, 416, 416))\n",
    "\n",
    "outputs = model(dummy_tensor)\n",
    "\n",
    "assert outputs[0].shape == (2, 3, 416 // 32, 416 // 32, 20 + 5)\n",
    "assert outputs[1].shape == (2, 3, 416 // 16, 416 // 16, 20 + 5)\n",
    "assert outputs[2].shape == (2, 3, 416 // 8, 416 // 8, 20 + 5)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac7ab6-0ed2-4ace-ab94-39568ccb0c08",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c71fda6-1fd6-40fc-90e6-da0a4679c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def _resize(image: np.ndarray, imsize=416) -> np.ndarray:\n",
    "    ratio = imsize / max(image.shape)\n",
    "    image = cv2.resize(image, (0, 0), fx=ratio, fy=ratio)\n",
    "    return image\n",
    "\n",
    "def _pad_to_square(image: np.ndarray) -> np.ndarray:\n",
    "    height, width = image.shape[:2]\n",
    "    max_size = max(height, width)\n",
    "    image = np.pad(image, ((0, max_size - height), (0, max_size - width), (0, 0)))\n",
    "    return image\n",
    "\n",
    "def preprocess(images, imsize=416, mean=[0, 0, 0], std=[1, 1, 1], device='cpu'):\n",
    "    mean = torch.tensor(mean, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(std, dtype=torch.float, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    samples = [_resize(image, imsize=imsize) for image in images]\n",
    "    samples = [_pad_to_square(sample) for sample in samples]\n",
    "    samples = [cv2.cvtColor(sample, cv2.COLOR_BGR2RGB) for sample in samples]\n",
    "    samples = [torch.from_numpy(sample) for sample in samples]\n",
    "    samples = torch.stack(samples, dim=0).to(device)\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = (samples.float().div(255.) - mean) / std\n",
    "\n",
    "    scales = [max(image.shape[:2]) / imsize for image in images]\n",
    "\n",
    "    return images, scales, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e819d4ff-17c1-43f1-bbe1-ab9ef0b1994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import ops\n",
    "\n",
    "def postprocess(preds, anchors, imsize=416, iou_threshold=0.5, score_threshold=0.05):\n",
    "    '''get all boxes at gird S x S (grid_size = imsize / S)\n",
    "    Args:\n",
    "        preds: Tuple[[N x 3 x S x S x (tp, tx, ty, tw, th, n_classes)]] with S = [13, 26, 52]\n",
    "        anchors: [3 x 3 x 2] (pw, ph with size in [0, 1])  (relative to imsize)\n",
    "    Outputs:\n",
    "        scores: [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "        labels: [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "        bboxes: [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3)) x 4], with [x1 y1 x2 y2].\n",
    "    '''\n",
    "    predictions = []\n",
    "    for i in range(3):\n",
    "        pred, anchor = preds[i], anchors[i]  # pred: N x 3 x S x S x (5 + C), anchor: 3 x 2\n",
    "        N, S, device = pred.shape[0], pred.shape[2], pred.device\n",
    "        \n",
    "        # anchor: 1 x 3 x 1 x 1 x 2\n",
    "        anchor = torch.tensor(anchor, device=device, dtype=torch.float)\n",
    "        anchor = anchor.reshape(1, 3, 1, 1, 2)\n",
    "\n",
    "        # N x num_anchors x S x S x 1\n",
    "        x_indices = torch.arange(S).repeat(N, 3, S, 1)\n",
    "        x_indices = x_indices.unsqueeze(dim=-1).to(device)\n",
    "        y_indices = x_indices.permute(0, 1, 3, 2, 4)\n",
    "\n",
    "        # N x num_anchors x S x S -> N x (num_anchors * S * S)\n",
    "        scores = torch.sigmoid(pred[..., 0]).reshape(N, 3 * S * S)\n",
    "\n",
    "        # N x num_anchors x S x S -> N x (num_anchors * S * S)\n",
    "        labels = torch.argmax(pred[..., 5:], dim=-1)\n",
    "        labels = labels.reshape(N, 3 * S * S)\n",
    "\n",
    "        # N x num_anchors x S x S x 2 -> N x (num_anchors * S * S) x 2\n",
    "        x = (torch.sigmoid(pred[..., 1:2]) + x_indices) * (imsize / S)\n",
    "        x = x.reshape(N, 3 * S * S, 1)\n",
    "        \n",
    "        y = (torch.sigmoid(pred[..., 2:3]) + y_indices) * (imsize / S)\n",
    "        y = y.reshape(N, 3 * S * S, 1)\n",
    "\n",
    "        xy = torch.cat([x, y], dim=-1)\n",
    "\n",
    "        # N x num_anchors x S x S x 2 -> N x (num_anchors * S * S) x 2\n",
    "        wh = anchor * torch.exp(pred[..., 3:5]) * imsize\n",
    "        wh = wh.reshape(N, 3 * S * S, 2)\n",
    "\n",
    "        # N x (num_anchors * S * S) x 4\n",
    "        boxes = torch.cat([xy - wh / 2, xy + wh / 2], dim=-1)\n",
    "        boxes[:, :, 0:1] = torch.clamp(boxes[:, :, 0:1], min=0)\n",
    "        boxes[:, :, 2:3] = torch.clamp(boxes[:, :, 2:3], max=imsize)\n",
    "\n",
    "        predictions.append([labels, scores, boxes])\n",
    "    \n",
    "    batch_labels = torch.cat([pred[0] for pred in predictions], dim=1)  # [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "    batch_scores = torch.cat([pred[1] for pred in predictions], dim=1)  # [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3))]\n",
    "    batch_boxes = torch.cat([pred[2] for pred in predictions], dim=1)  # [N x (3 * (S1 * S1 + S2 * S2 + S3 * S3)) x 4]\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(preds[0].shape[0]):\n",
    "        scores_over_thresh = batch_scores[i, :] > score_threshold\n",
    "        if scores_over_thresh.sum() == 0:\n",
    "            predictions.append({'boxes': torch.FloatTensor([[0, 0, 1, 1]]),\n",
    "                                'labels': torch.FloatTensor([-1]),\n",
    "                                'scores': torch.FloatTensor([0])})\n",
    "            continue\n",
    "\n",
    "        labels = batch_labels[i, scores_over_thresh]\n",
    "        boxes = batch_boxes[i, scores_over_thresh, :]\n",
    "        scores = batch_scores[i, scores_over_thresh]\n",
    "\n",
    "        nms_idx = ops.boxes.batched_nms(boxes, scores, labels, iou_threshold=iou_threshold)\n",
    "\n",
    "        if nms_idx.shape[0] != 0:\n",
    "            labels = labels[nms_idx]\n",
    "            scores = scores[nms_idx]\n",
    "            boxes = boxes[nms_idx, :]\n",
    "\n",
    "            predictions.append({'boxes': boxes, 'labels': labels, 'scores': scores})\n",
    "        else:\n",
    "            predictions.append({'boxes': torch.FloatTensor([[0, 0, 1, 1]]),\n",
    "                                'labels': torch.FloatTensor([-1]),\n",
    "                                'scores': torch.FloatTensor([0])})\n",
    "\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdd8388-946f-433e-809f-6140d64bbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained weight\n",
    "model = YOLOv3(num_classes=20)\n",
    "state_dict = torch.load(f='checkpoint/pretrained_weight/78.1map_0.2threshold_PASCAL.tar', map_location='cpu')\n",
    "model.load_state_dict(state_dict=state_dict['state_dict'])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcd8ec13-9522-43ee-b564-43db239762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "image_paths = [\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000005.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000012.jpg',\n",
    "    '../efficient_det_pytorch/dataset/PASCALVOC2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/000016.jpg',\n",
    "]\n",
    "\n",
    "images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "\n",
    "images, scales, samples = preprocess(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a191a66-9ec9-461a-9aa7-f3f8963e6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([3, 3, 416, 416])\n",
      "Output Shape at S=13: torch.Size([3, 3, 13, 13, 25])\n",
      "Output Shape at S=26: torch.Size([3, 3, 26, 26, 25])\n",
      "Output Shape at S=52: torch.Size([3, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "with torch.no_grad():\n",
    "    preds = model(samples)\n",
    "\n",
    "print(f'Input Shape: {samples.shape}')    \n",
    "print(f'Output Shape at S={preds[0].shape[2]}: {preds[0].shape}')\n",
    "print(f'Output Shape at S={preds[1].shape[2]}: {preds[1].shape}')\n",
    "print(f'Output Shape at S={preds[2].shape[2]}: {preds[2].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98f66f1d-73f7-47d4-94e2-dc086f6d76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing\n",
    "iou_threshold = 0.45\n",
    "score_threshold = 0.05\n",
    "\n",
    "imsize = 416\n",
    "S = [13, 26, 52]\n",
    "anchors = [[(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],    # S = 13\n",
    "           [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],   # S = 26\n",
    "           [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],]  # S = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b05a451-896a-42a7-9349-842c6d2fc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = postprocess(preds=preds, anchors=anchors, imsize=imsize, iou_threshold=0.5, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b52bfbe-22dc-4cbe-be81-7ea9289c2280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[135.5854, 210.2635, 222.2740, 310.1468],\n",
       "          [204.4330, 161.3284, 261.8168, 271.7595],\n",
       "          [  4.1186, 195.3734,  63.7083, 307.3726],\n",
       "          [229.5404, 154.1949, 261.5659, 182.4224],\n",
       "          [261.1333, 149.7752, 289.5691, 179.2450],\n",
       "          [194.2008, 159.0178, 245.7994, 249.7052],\n",
       "          [179.5570, 156.9499, 226.7502, 219.2694],\n",
       "          [216.1679, 159.5498, 269.2601, 233.0432],\n",
       "          [231.1824, 154.0324, 254.3342, 173.7911],\n",
       "          [279.5289, 152.8962, 305.2201, 177.5389],\n",
       "          [377.7064, 150.8492, 413.7950, 320.2241],\n",
       "          [253.7750, 155.2566, 282.9843, 186.1714],\n",
       "          [378.3737, 141.9657, 413.5199, 313.8051],\n",
       "          [220.0476, 154.6475, 266.1848, 203.0758]]),\n",
       "  'labels': tensor([ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 14,  8]),\n",
       "  'scores': tensor([0.8458, 0.8184, 0.8114, 0.7521, 0.7256, 0.6656, 0.6551, 0.6264, 0.5843,\n",
       "          0.5616, 0.5572, 0.5501, 0.5443, 0.5120])},\n",
       " {'boxes': tensor([[126.4862,  70.2691, 286.6494, 227.7021]]),\n",
       "  'labels': tensor([6]),\n",
       "  'scores': tensor([0.8998])},\n",
       " {'boxes': tensor([[ 72.8772,  36.9785, 282.7292, 398.4085]]),\n",
       "  'labels': tensor([1]),\n",
       "  'scores': tensor([0.8798])}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df470f41-bb44-4306-b568-67ab2b7371d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2idx = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "               'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10,\n",
    "               'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15,\n",
    "               'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "classes = list(classes2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25cbf58d-770c-4b96-9714-b28e5aa2c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, scale, pred in zip(images, scales, predictions):\n",
    "    thickness = max(image.shape) // 600\n",
    "    fontscale = max(image.shape) / 700\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    labels = pred['labels'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "    class_names = [classes[label] for label in labels]\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale\n",
    "    boxes = boxes.astype(np.int32)\n",
    "    for box, score, class_name in zip(boxes, scores, class_names):\n",
    "        color = (np.random.randint(200, 255),\n",
    "                 np.random.randint(50, 200),\n",
    "                 np.random.randint(0, 150))\n",
    "#         if label != -1:\n",
    "        cv2.rectangle(\n",
    "            img=image,\n",
    "            pt1=tuple(box[:2]),\n",
    "            pt2=tuple(box[2:]),    \n",
    "            color=color,\n",
    "            thickness=thickness\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            img=image,\n",
    "            text=f'{class_name}: {score: .4f}',\n",
    "            org=tuple(box[:2]),\n",
    "            fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "            fontScale=fontscale,\n",
    "            color=color,\n",
    "            thickness=thickness,\n",
    "            lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(class_name, image)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28617881-d44e-4ba2-9584-017cd8df3b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
